#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Model LLM wrapper with optimized settings for speed & stability
- ∆Øu ti√™n model merged n·∫øu c√≥; fallback sang base
- H·ªó tr·ª£ Transformers (PhoGPT/MPT) v√† Ollama
- Fallback khi thi·∫øu file local (tokenizer/model)
"""

from typing import List, Optional
import os, re, requests

# Try to import transformers/torch (optional at import-time)
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except Exception:
    TRANSFORMERS_AVAILABLE = False

# ONNX: ch∆∞a d√πng (placeholder)
ONNX_AVAILABLE = False

# =========================
# Base LLM class
# =========================
class BaseLLM:
    def generate(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        raise NotImplementedError

# =========================
# Helpers
# =========================
def _is_local_path(p: str) -> bool:
    if not p:
        return False
    # Windows drive (C:\...), absolute Unix (/ or \), or existing directory
    return os.path.isdir(p) or bool(re.match(r"^[A-Za-z]:\\", p)) or p.startswith(('/', '\\'))

def _enable_cuda_fastmath():
    try:
        if torch.cuda.is_available():
            # Cho ph√©p TF32 (tr√™n Ampere+)
            try:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
            except Exception:
                pass
            # PyTorch >= 2.0
            try:
                torch.set_float32_matmul_precision("high")
            except Exception:
                pass
    except Exception:
        pass

def _pick_dtype(device: str):
    if device == "cuda" and torch.cuda.is_available():
        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    return torch.float32

def _safe_eos_id(tokenizer) -> Optional[int | List[int]]:
    try:
        # C√≥ model.generation_config.eos_token_id c√≥ th·ªÉ l√† int ho·∫∑c list
        return getattr(tokenizer, "eos_token_id", None) or getattr(tokenizer, "eos_token", None)
    except Exception:
        return None

# =========================
# Transformers LLM
# =========================
class TransformersLLM(BaseLLM):
    def __init__(
        self,
        model_path: str,
        device: str = "cpu",
        temperature: float = 0.01,
        max_new_tokens: int = 32,
        local_files_only: bool = True,
        clear_cache_first: bool = False,
        allow_ignore_mismatch: bool = False,
    ):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers/torch is not available.")

        # Ch·ªçn device
        self.device = "cuda" if (device == "cuda" and torch.cuda.is_available()) else "cpu"
        self.temperature = float(temperature)
        self.max_new_tokens = int(max_new_tokens)
        self.local_files_only = bool(local_files_only)
        self.clear_cache_first = bool(clear_cache_first)
        self.allow_ignore_mismatch = bool(allow_ignore_mismatch)

        # LATEST MERGE: S·ª≠ d·ª•ng model merged m·ªõi nh·∫•t
        latest_model_path = r"D:\AI.LLM-khanh-no_rrf\models\phogpt4b-ft-merged"

        print("=" * 60)
        print("üîç MODEL SELECTION")
        print("=" * 60)

        # Ch·ªâ s·ª≠ d·ª•ng latest merged model
        if os.path.exists(latest_model_path) and os.path.exists(os.path.join(latest_model_path, "config.json")):
            self.model_path = latest_model_path
            print(f"üéØ USING: LATEST MERGED MODEL")
            print(f"üìÇ Path: {self.model_path}")
            print(f"‚≠ê Benefits: Most recent merge, optimized stability, anti-garbled")
        else:
            raise FileNotFoundError(f"‚ùå Latest merged model not found at: {latest_model_path}")
            
        print("=" * 60)
        print(f"‚ö° Loading model {self.model_path} on {self.device}, max_new={self.max_new_tokens}, temp={self.temperature}")

        # Tu·ª≥ ch·ªçn d·ªçn cache (th·ª±c s·ª± ch·ªâ n√™n d√πng khi g·∫∑p xung ƒë·ªôt)
        if self.clear_cache_first:
            try:
                import shutil
                cache_dir = os.path.expanduser("~/.cache/huggingface")
                print(f"üßπ Clearing HuggingFace cache (selected folders)...")
                # Kh√¥ng x√≥a to√†n b·ªô; ch·ªâ modules/transformers_modules ƒë·ªÉ tr√°nh ƒë·ª•ng model repo cache
                modules_dir = os.path.join(cache_dir, "modules", "transformers_modules")
                if os.path.exists(modules_dir):
                    shutil.rmtree(modules_dir, ignore_errors=True)
                    print(f"üßπ Removed: {modules_dir}")
            except Exception as cache_err:
                print(f"‚ö†Ô∏è Cache clear warning: {cache_err}")

        # CUDA fastmat
        _enable_cuda_fastmath()

        # Ch·ªçn dtype
        dtype = _pick_dtype(self.device)

        # Load tokenizer (ch·ªâ t·ª´ stable merged model)
        print(f"üîÑ Loading tokenizer from {self.model_path}...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=False,  # Stable model kh√¥ng c·∫ßn trust_remote_code
                local_files_only=self.local_files_only,
                use_fast=True,
                padding_side="left",
                cache_dir=None
            )
        except Exception as e_tok:
            if self.local_files_only:
                print(f"‚ö†Ô∏è Tokenizer local load failed: {e_tok}. Retrying with local_files_only=False...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    trust_remote_code=False,  # V·∫´n gi·ªØ False cho stable
                    local_files_only=False,
                    use_fast=True,
                    padding_side="left",
                )
            else:
                raise

        # Tokenizer info
        print(f"üìã Tokenizer info:")
        try:
            print(f"  - Type: {type(self.tokenizer).__name__}")
            print(f"  - Vocab size: {getattr(self.tokenizer, 'vocab_size', 'Unknown')}")
            print(f"  - Model max length: {getattr(self.tokenizer, 'model_max_length', 'Unknown')}")
        except Exception:
            pass

        # Set pad token
        if getattr(self.tokenizer, "pad_token", None) is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print(f"  - Set pad_token to eos_token: {self.tokenizer.eos_token}")
        else:
            print(f"  - Existing pad_token: {self.tokenizer.pad_token}")

        # Quick VN encode/decode self-test
        try:
            test_text = "ƒê√®n giao th√¥ng c√≥ ba m√†u"
            test_tokens = self.tokenizer.encode(test_text, add_special_tokens=False)
            test_decoded = self.tokenizer.decode(test_tokens)
            print(f"  - Test encode/decode: OK" if test_decoded.strip() == test_text.strip() else "  ‚ö†Ô∏è Tokenizer encode/decode mismatch!")
        except Exception:
            print("  ‚ö†Ô∏è Tokenizer self-test skipped")

        print("‚úÖ Tokenizer loaded")

        # Load model (latest merged model v·ªõi settings t·ªëi ∆∞u)
        print(f"üîÑ Loading model from {self.model_path}...")
        
        # Latest model kh√¥ng c·∫ßn flash_attn_triton.py
        print(f"üîç Using latest merged model - no flash attention needed")
        
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                trust_remote_code=False,      # Latest model kh√¥ng c·∫ßn trust remote code
                local_files_only=self.local_files_only,
                torch_dtype=dtype,
                low_cpu_mem_usage=True,
                use_cache=True,
                device_map="auto" if self.device == "cuda" else None,
                revision=None,
                ignore_mismatched_sizes=self.allow_ignore_mismatch,
                cache_dir=None
            )
            print(f"‚úÖ Latest merged model loaded successfully")
        except Exception as e:
            print(f"‚ùå Error loading latest model: {e}")
            raise RuntimeError(f"Failed to load latest merged model: {e}")

        # Kh√¥ng √©p .to("cuda") n·∫øu ƒë√£ d√πng device_map="auto"
        if self.device == "cuda" and getattr(self.model, "hf_device_map", None) is None:
            # Model ch∆∞a ƒë∆∞·ª£c map t·ª± ƒë·ªông (v√≠ d·ª• device="cpu") ‚Üí c√≥ th·ªÉ move
            try:
                self.model.to("cuda")
            except Exception:
                pass

        self.model.eval()
        try:
            torch.set_grad_enabled(False)
        except Exception:
            pass

        # Th√¥ng tin model
        print("=" * 60)
        print("üìä MODEL INFORMATION")
        print("=" * 60)
        print(f"üè∑Ô∏è  Model Name: {os.path.basename(self.model_path)}")
        print(f"üìÇ Model Path: {self.model_path}")
        print(f"üîß Model Type: {type(self.model).__name__}")
        print(f"üíæ Device: {self.device}")
        print(f"üî¢ Data Type: {dtype}")
        print(f"‚≠ê Status: LATEST MERGED MODEL (newest version, anti-garbled)")
        print("=" * 60)

        # L∆∞u EOS/Pad ID an to√†n
        self._eos_token_id = getattr(self.tokenizer, "eos_token_id", None)
        self._pad_token_id = getattr(self.tokenizer, "pad_token_id", None)

    def _tokenize_prompt(self, prompt: str):
        # T√≠nh to√°n max input an to√†n: model_max_length - max_new_tokens - margin
        model_max = int(getattr(self.tokenizer, "model_max_length", 2048) or 2048)
        margin = 16
        max_input_len = max(32, model_max - self.max_new_tokens - margin)

        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=max_input_len,
            padding=False
        )
        if self.device == "cuda":
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        return inputs

    def generate(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        try:
            # Debug input
            print(f"üî§ Input prompt: '{prompt[:200]}{'...' if len(prompt) > 200 else ''}'")

            inputs = self._tokenize_prompt(prompt)
            input_len = inputs["input_ids"].shape[1]
            print(f"üî¢ Input tokens: {input_len}")

            # Ch·ªçn eos/pad id
            eos_token_id = self._eos_token_id
            pad_token_id = self._pad_token_id

            gen_kwargs = dict(
                max_new_tokens=self.max_new_tokens,
                do_sample=False,
                pad_token_id=pad_token_id,
                eos_token_id=eos_token_id,
                num_beams=1,
                repetition_penalty=1.15,
                no_repeat_ngram_size=3,
                use_cache=True,
                return_dict_in_generate=False,
            )

            with torch.inference_mode():
                outputs = self.model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask"),
                    **gen_kwargs
                )

            seq = outputs[0] if getattr(outputs, "dim", lambda: 2)() == 2 else outputs
            new_ids = seq[input_len:]
            print(f"üéØ Generated token count: {len(new_ids)}")

            response = self.tokenizer.decode(
                new_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            ).strip()

            # Clean-up v·ªõi ki·ªÉm so√°t
            response = sanitize(response)

            # Stop tokens (n·∫øu c√≥)
            if stop:
                for st in stop:
                    idx = response.find(st)
                    if idx != -1:
                        response = response[:idx].strip()
                        break

            # Ki·ªÉm tra t·ªëi thi·ªÉu
            if len(response) < 3 or len(set(response.replace(' ', ''))) < 3:
                return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

            return response
        except Exception as e:
            return f"[ERROR] {e}"

# =========================
# Ollama client
# =========================
class OllamaGenerateLLM(BaseLLM):
    def __init__(
        self,
        model_id: str = None,
        host: str = None,
        temperature: float = 0.05,
        top_p: float = 0.95,
        max_new_tokens: int = 64,
        timeout_connect: int = 10,
        timeout_read: int = 120,
        keep_alive: str = "5m"
    ):
        self.model_id = model_id or os.getenv("LLM_MODEL_ID", "mrjacktung/phogpt-4b-chat-gguf:latest")
        self.host = (host or os.getenv("LLM_OLLAMA_HOST", "http://localhost:11434")).rstrip("/")
        self.temperature = float(temperature)
        self.top_p = float(top_p)
        self.max_new_tokens = int(max_new_tokens)
        self.timeout_connect = int(timeout_connect)
        self.timeout_read = int(timeout_read)
        self.keep_alive = keep_alive

    def set_params(self, temperature: float = None, max_new_tokens: int = None):
        if temperature is not None:
            self.temperature = float(temperature)
        if max_new_tokens is not None:
            self.max_new_tokens = int(max_new_tokens)

    def generate(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        url = f"{self.host}/api/generate"
        payload = {
            "model": self.model_id,
            "prompt": prompt,
            "stream": False,
            "keep_alive": self.keep_alive,
            "options": {
                "temperature": self.temperature,
                "top_p": self.top_p,
                "num_predict": self.max_new_tokens,
            },
        }
        if stop:
            payload["stop"] = stop
        try:
            print(f"üõ†Ô∏è Ollama call: model={self.model_id}, host={self.host}")
            r = requests.post(url, json=payload, timeout=(self.timeout_connect, self.timeout_read))
            r.raise_for_status()
            data = r.json()
            return (data.get("response") or "").strip()
        except Exception as e:
            return f"[Ollama ERROR] {e}"

# =========================
# Builder
# =========================
def build_llm(
    model_path_or_id,
    host=None,
    temperature=0.05,
    device="cpu",
    use_onnx=True,
    **kwargs
):
    # C·∫£nh b√°o ONNX ch∆∞a b·∫≠t
    if use_onnx and not ONNX_AVAILABLE:
        print("‚ÑπÔ∏è ONNX path requested but not enabled yet. Using Transformers/Ollama.")

    max_new_tokens = int(kwargs.get("max_new_tokens", 48))
    local_files_only = bool(kwargs.get("local_files_only", True))
    clear_cache_first = bool(kwargs.get("clear_cache_first", False))
    allow_ignore_mismatch = bool(kwargs.get("allow_ignore_mismatch", False))

    if model_path_or_id and _is_local_path(model_path_or_id):
        print("üì¶ Using Transformers (PhoGPT/MPT).")
        return TransformersLLM(
            model_path=model_path_or_id,
            device=device,
            temperature=temperature,
            max_new_tokens=max_new_tokens,
            local_files_only=local_files_only,
            clear_cache_first=clear_cache_first,
            allow_ignore_mismatch=allow_ignore_mismatch,
        )
    else:
        print("üåê Using Ollama backend.")
        return OllamaGenerateLLM(
            model_id=model_path_or_id,
            host=host,
            temperature=temperature,
            max_new_tokens=max_new_tokens,
        )

# =========================
# STOP tokens & sanitize
# =========================
STOP_TOKENS = [
    "<sources>", "</sources>",
    "<bm25_hints>", "</bm25_hints>",
    "<bm25_only>", "</bm25_only>",
    "<question>", "</question>",
    "<sample_answer>", "</sample_answer>",
    "<theory>", "</theory>",
    "<given_answer>", "</given_answer>",
    "Traceback", "Error", "ERROR",
    "‚ñ™", "‚ñ´", "‚Ä¢", "‚ñ°", "‚óä",
    "...", "‚Ä¶",
    "</s>", "<s>", "[INST]", "[/INST]",
]

_TAGS_RE = re.compile(
    r"</?(sources|bm25_hints|bm25_only|question|sample_answer|theory|given_answer)>",
    re.IGNORECASE
)
_BM25_LINE_RE = re.compile(r"^\s*\[B\d+\].*$", flags=re.MULTILINE)

def sanitize(text: str) -> str:
    s = text if isinstance(text, str) else (str(text) if text is not None else "")
    # Lo·∫°i tag & hint debug
    s = _BM25_LINE_RE.sub("", s)
    s = _TAGS_RE.sub("", s)

    # K√Ω t·ª± l·ªói th∆∞·ªùng g·∫∑p
    s = re.sub(r'[ÔøΩ‚ñ°‚óä‚ñ™‚ñ´‚Ä¢‚Ä∞‚Ä¶\ufffd]', '', s)

    # G·ªôp t·ª´ l·∫∑p li·ªÅn nhau
    s = re.sub(r'\b(\w+)\s+\1\b', r'\1', s, flags=re.IGNORECASE)

    # M·ªôt s·ªë r√°c in/ti·ªÅn x·ª≠ l√Ω (gi·ªØ an to√†n VN)
    s = s.replace("c·ªông  c·ªông", " ").replace("c·ªông c·ªông", " ")
    # Kh√¥ng xo√° ‚Äúc·ªông‚Äù ƒë∆°n l·∫ª ƒë·ªÉ tr√°nh ƒÉn nh·∫ßm t·ª´ h·ª£p l·ªá (C·ªông ho√†, c·ªông t√°c, c·ªông ƒë·ªìng, ...)

    s = re.sub(r'\s+', ' ', s).strip()
    return s

# =========================
# Prompt templates
# =========================
def build_prompt_ref_with_sample(user_question: str, theory_block: str, sample_answer: Optional[str] = None) -> str:
    sample = (sample_answer or "").strip()
    prompt = f"Th√¥ng tin: {theory_block.strip()}\n\n"
    if sample:
        prompt += f"M·∫´u: {sample}\n\n"
    prompt += f"H·ªèi: {user_question.strip()}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c:"
    return prompt

def build_prompt_with_context(user_question: str, theory_block: str) -> str:
    return f"Th√¥ng tin: {theory_block.strip()}\n\nH·ªèi: {user_question.strip()}\nTr·∫£ l·ªùi ng·∫Øn:"

def build_prompt_with_sample(user_question: str, sample_answer: str) -> str:
    return f"M·∫´u: {sample_answer.strip()}\n\nH·ªèi: {user_question.strip()}\nTr·∫£ l·ªùi t∆∞∆°ng t·ª±:"

def build_prompt_direct(user_question: str) -> str:
    return f"H·ªèi: {user_question.strip()}\nTr·∫£ l·ªùi ng·∫Øn:"

# =========================
# Generate wrapper
# =========================
def generate_answer(
    llm: BaseLLM,
    question: str,
    context: str,
    max_new_tokens: int = 64,
    temperature: Optional[float] = None
) -> str:
    try:
        prompt = build_prompt_ref_with_sample(question, context)
        return llm.generate(prompt, stop=STOP_TOKENS)
    except Exception as e:
        return f"[Generation ERROR] {e}"